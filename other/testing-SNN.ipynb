{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#import environments\n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "# import skimage\n",
    "# from skimage import data\n",
    "# from skimage import transform\n",
    "# import os \n",
    "\n",
    "from alif_functions import CustomALIFWithReset, spike_function, CustomALIF\n",
    "from util import to_bool, switch_time_and_batch_dimension, exp_convolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def spike_function(v_scaled, dampening_factor):\n",
    "    z_ = tf.greater(v_scaled, 0.)\n",
    "    z_ = tf.cast(z_, dtype=tf.float32)\n",
    "\n",
    "    def grad(dy):\n",
    "        dE_dz = dy\n",
    "        dz_dv_scaled = tf.maximum(1 - tf.abs(v_scaled), 0)\n",
    "        dz_dv_scaled *= dampening_factor\n",
    "\n",
    "        dE_dv_scaled = dE_dz * dz_dv_scaled\n",
    "\n",
    "        return [dE_dv_scaled,\n",
    "                tf.zeros_like(dampening_factor)]\n",
    "\n",
    "    return tf.identity(z_, name=\"spike_function\"), grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_dynamic(v, i, decay, v_th, dampening_factor=.3):\n",
    "    old_z = spike_function((v - v_th) / v_th, dampening_factor)\n",
    "    new_v = decay * v + i - old_z * v_th\n",
    "    new_z = spike_function((new_v - v_th) / v_th, dampening_factor)\n",
    "    return new_v, new_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingCNN(tf.compat.v1.nn.rnn_cell.RNNCell):\n",
    "    def __init__(self, n_kernel_1=8, n_filter_1=16, stride_1=4, n_kernel_2=8, n_filter_2=32, stride_2=4, ba=False,\n",
    "\t\t\t\t avg_ba=False, ba_config=None, tau=1, thr=1., avg_pool_1_stride=4, avg_pool_1_k=8, avg_pool_2_stride=2,\n",
    "\t\t\t\t avg_pool_2_k=4):\n",
    "        super().__init__()\n",
    "        self.decay = np.exp(-1 / tau)\n",
    "        self.v_th = thr\n",
    "        self.n_filters_1 = n_filter_1\n",
    "        self.n_filters_2 = n_filter_2\n",
    "        self.ba = ba\n",
    "        self.avg_ba = avg_ba\n",
    "        self.n_w_1 = (128 - n_kernel_1) // stride_1 + 2\n",
    "        self.n_w_2 = (self.n_w_1 - n_kernel_2) // stride_2 + 1\n",
    "        self.n_kernel_1 = n_kernel_1\n",
    "        self.stride_1 = stride_1\n",
    "        self.n_kernel_2 = n_kernel_2\n",
    "        self.stride_2 = stride_2\n",
    "        self.avg_pool_1_stride = avg_pool_1_stride\n",
    "        self.avg_pool_2_stride = avg_pool_2_stride\n",
    "        self.avg_pool_1_k = avg_pool_1_k\n",
    "        self.avg_pool_2_k = avg_pool_2_k\n",
    "\n",
    "        self.n_avg_1 = (self.n_w_1 - avg_pool_1_k) // avg_pool_1_stride + 1\n",
    "        self.n_avg_2 = (self.n_w_2 - avg_pool_2_k) // avg_pool_2_stride + 1\n",
    "\n",
    "        if ba_config is not None:\n",
    "            self.ba_filters_1_1 = ba_config['ba_filters_1_1']\n",
    "            self.ba_kernel_1_1 = ba_config['ba_kernel_1_1']\n",
    "            self.ba_stride_1_1 = ba_config['ba_stride_1_1']\n",
    "            self.ba_filters_1_2 = ba_config['ba_filters_1_2']\n",
    "            self.ba_kernel_1_2 = ba_config['ba_kernel_1_2']\n",
    "            self.ba_stride_1_2 = ba_config['ba_stride_1_2']\n",
    "            self.ba_filters_2 = ba_config['ba_filters_2']\n",
    "            self.ba_kernel_2 = ba_config['ba_kernel_2']\n",
    "            self.ba_stride_2 = ba_config['ba_stride_2']\n",
    "        else:\n",
    "            self.ba_filters_1_1 = 16\n",
    "            self.ba_kernel_1_1 = 8\n",
    "            self.ba_stride_1_1 = 4\n",
    "            self.ba_filters_1_2 = 32\n",
    "            self.ba_kernel_1_2 = 4\n",
    "            self.ba_stride_1_2 = 2\n",
    "            self.ba_filters_2 = 32\n",
    "            self.ba_kernel_2 = 4\n",
    "            self.ba_stride_2 = 2\n",
    "\n",
    "        self.n_ba_1 = (self.n_w_1 - self.ba_kernel_1_1) // self.ba_stride_1_1 + 1\n",
    "        self.n_ba_2 = (self.n_w_2 - self.ba_kernel_2) // self.ba_stride_2 + 1\n",
    "\n",
    "        # self.n_ba_1 = (self.n_w_1 - ba_config['ba_kernel_1_1']) // ba_config['ba_stride_1_1'] + 1\n",
    "        # self.n_ba_2 = (self.n_w_2 - ba_config['ba_kernel_2']) // ba_config['ba_stride_2'] + 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        if self.ba:\n",
    "            return self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "                   self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "                   self.n_w_2 * self.n_w_2 * self.n_filters_2, \\\n",
    "                   self.n_w_2 * self.n_w_2 * self.n_filters_2, \\\n",
    "                   self.n_ba_1 * self.n_ba_1 * self.ba_filters_1_1, \\\n",
    "                   self.n_ba_2 * self.n_ba_2 * self.ba_filters_2\n",
    "        if self.avg_ba:\n",
    "            return self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "                   self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "                   self.n_w_2 * self.n_w_2 * self.n_filters_2, \\\n",
    "                   self.n_w_2 * self.n_w_2 * self.n_filters_2, \\\n",
    "                   self.n_avg_1 * self.n_avg_1 * self.n_filters_1, \\\n",
    "                   self.n_avg_2 * self.n_avg_2 * self.n_filters_2\n",
    "        return self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "               self.n_w_1 * self.n_w_1 * self.n_filters_1, \\\n",
    "               self.n_w_2 * self.n_w_2 * self.n_filters_2, \\\n",
    "               self.n_w_2 * self.n_w_2 * self.n_filters_2\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        return tf.zeros((batch_size, self.n_w_1, self.n_w_1, self.n_filters_1), dtype), \\\n",
    "               tf.zeros((batch_size, self.n_w_2, self.n_w_2, self.n_filters_2), dtype)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (self.n_w_1, self.n_w_1, self.n_filters_1), (self.n_w_2, self.n_w_2, self.n_filters_2)\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        v_conv_1, z_conv_1 = lif_dynamic(state[0], inputs, self.decay, self.v_th, 1.)\n",
    "        if self.ba and not self.avg_ba:\n",
    "            with tf.compat.v1.variable_scope('broadcast_1'):\n",
    "                c1_r = snt.Conv2D(\n",
    "                    self.ba_filters_1_1,\n",
    "                    self.ba_kernel_1_1,\n",
    "                    stride=self.ba_stride_1_1,\n",
    "                    padding='VALID'\n",
    "                )(z_conv_1)\n",
    "                c1_r = snt.BatchFlatten()(c1_r)\n",
    "\n",
    "                z_conv_1 = tf.stop_gradient(z_conv_1)\n",
    "        elif self.avg_ba:\n",
    "            with tf.compat.v1.variable_scope('broadcast_1'):\n",
    "                c1_r = tf.nn.avg_pool2d(\n",
    "                    input=z_conv_1, ksize=self.avg_pool_1_k, \n",
    "                    strides=self.avg_pool_1_stride, padding='VALID')\n",
    "                c1_r = snt.BatchFlatten()(c1_r)\n",
    "                z_conv_1 = tf.stop_gradient(z_conv_1)\n",
    "\n",
    "        i_conv_2 = snt.Conv2D(self.n_filters_2, self.n_kernel_2, stride=self.stride_2, padding='VALID')(z_conv_1)\n",
    "        v_conv_2, z_conv_2 = lif_dynamic(state[1], i_conv_2, self.decay, self.v_th, 1.)\n",
    "        if self.ba and not self.avg_ba:\n",
    "            with tf.compat.v1.variable_scope('broadcast_2'):\n",
    "                layer_c2_r = snt.Conv2D(\n",
    "                    self.ba_filters_2,\n",
    "                    self.ba_kernel_2,\n",
    "                    stride=self.ba_stride_2,\n",
    "                    padding='VALID'\n",
    "                )\n",
    "                c2_r = layer_c2_r(z_conv_2)\n",
    "                c2_r = snt.BatchFlatten()(c2_r)\n",
    "\n",
    "                z_conv_2 = tf.stop_gradient(z_conv_2)\n",
    "        elif self.avg_ba:\n",
    "            with tf.compat.v1.variable_scope('broadcast_2'):\n",
    "                c2_r = tf.nn.avg_pool2d(\n",
    "                    input=z_conv_2, ksize=self.avg_pool_2_k, \n",
    "                    strides=self.avg_pool_2_stride, padding='VALID')\n",
    "                c2_r = snt.BatchFlatten()(c2_r)\n",
    "                z_conv_2 = tf.stop_gradient(z_conv_2)\n",
    "        new_state = (v_conv_1, v_conv_2)\n",
    "        if self.ba or self.avg_ba:\n",
    "            return (tf.reshape(z_conv_1, (-1, self.n_w_1 * self.n_w_1 * self.n_filters_1)),\n",
    "                    tf.reshape(v_conv_1, (-1, self.n_w_1 * self.n_w_1 * self.n_filters_1)),\n",
    "                    tf.reshape(z_conv_2, (-1, self.n_w_2 * self.n_w_2 * self.n_filters_2)),\n",
    "                    tf.reshape(v_conv_2, (-1, self.n_w_2 * self.n_w_2 * self.n_filters_2)), c1_r, c2_r), new_state\n",
    "        return (tf.reshape(z_conv_1, (-1, self.n_w_1 * self.n_w_1 * self.n_filters_1)),\n",
    "                tf.reshape(v_conv_1, (-1, self.n_w_1 * self.n_w_1 * self.n_filters_1)),\n",
    "                tf.reshape(z_conv_2, (-1, self.n_w_2 * self.n_w_2 * self.n_filters_2)),\n",
    "                tf.reshape(v_conv_2, (-1, self.n_w_2 * self.n_w_2 * self.n_filters_2))), new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingAgent(snt.RNNCore):\n",
    "    def __init__(self, action_set, rnn_units, stop_gradient=False, n_rnn_step_factor=1,\n",
    "                 tau=20, tau_readout=5, thr=.615, n_filters_1=16, n_filters_2=32,\n",
    "                 n_kernel_1=8, n_kernel_2=4, stride_1=4, stride_2=2,\n",
    "                 beta=.16, tau_adaptation=300, ba=False, avg_ba=False,\n",
    "                 ba_config=None, fraction_adaptive=.4, n_refractory=3,\n",
    "                 tau_scnn=1., thr_scnn=1., avg_pool_1_stride=4, avg_pool_1_k=8,\n",
    "                 avg_pool_2_stride=2, avg_pool_2_k=4):\n",
    "        super(SpikingAgent, self).__init__(name='agent')\n",
    "\n",
    "        self._num_actions = len(action_set)\n",
    "        self.ba = ba\n",
    "        self.avg_ba = avg_ba\n",
    "        self.rnn_units = rnn_units\n",
    "        tau_readout = np.atleast_1d(tau_readout)\n",
    "        self.decay = np.exp(-1 / tau_readout)\n",
    "        self.n_rnn_step_factor = n_rnn_step_factor\n",
    "        self.thr = thr\n",
    "        self.n_filters_1 = n_filters_1\n",
    "        self.n_kernel_1 = n_kernel_1\n",
    "        self.stride_1 = stride_1\n",
    "        self.n_filters_2 = n_filters_2\n",
    "        self.n_kernel_2 = n_kernel_2\n",
    "        self.stride_2 = stride_2\n",
    "        self.no_linear = True\n",
    "        if ba_config is not None:\n",
    "            self.ba_filters_1_1 = ba_config['ba_filters_1_1']\n",
    "            self.ba_kernel_1_1 = ba_config['ba_kernel_1_1']\n",
    "            self.ba_stride_1_1 = ba_config['ba_stride_1_1']\n",
    "            self.ba_filters_1_2 = ba_config['ba_filters_1_2']\n",
    "            self.ba_kernel_1_2 = ba_config['ba_kernel_1_2']\n",
    "            self.ba_stride_1_2 = ba_config['ba_stride_1_2']\n",
    "            self.ba_filters_2 = ba_config['ba_filters_2']\n",
    "            self.ba_kernel_2 = ba_config['ba_kernel_2']\n",
    "            self.ba_stride_2 = ba_config['ba_stride_2']\n",
    "        else:\n",
    "            self.ba_filters_1_1 = 32\n",
    "            self.ba_kernel_1_1 = 8\n",
    "            self.ba_stride_1_1 = 4\n",
    "            self.ba_filters_1_2 = 64\n",
    "            self.ba_kernel_1_2 = 4\n",
    "            self.ba_stride_1_2 = 2\n",
    "            self.ba_filters_2 = 64\n",
    "            self.ba_kernel_2 = 4\n",
    "            self.ba_stride_2 = 2\n",
    "\n",
    "        with self._enter_variable_scope():\n",
    "            n_regular = int(rnn_units * (1. - fraction_adaptive))\n",
    "            n_adaptive = rnn_units - n_regular\n",
    "            beta = np.concatenate((np.zeros(n_regular), np.ones(n_adaptive))).astype(np.float32) * beta\n",
    "            self.beta = beta\n",
    "            n_w_1 = (128 - n_kernel_1) // stride_1 + 2\n",
    "            n_w_2 = (n_w_1 - n_kernel_2) // stride_2 + 1\n",
    "            n_input = n_w_2 * n_w_2 * n_filters_2\n",
    "\n",
    "            #rnn_units can be experimented with \n",
    "\n",
    "            self.core = CustomALIFWithReset(n_input, rnn_units, tau=tau, beta=beta, thr=thr,\n",
    "                                            tau_adaptation=tau_adaptation, stop_gradients=stop_gradient,\n",
    "                                            n_refractory=n_refractory)\n",
    "            \n",
    "            self.scnn = SpikingCNN(n_filter_1=n_filters_1, stride_1=stride_1, n_kernel_1=n_kernel_1, n_filter_2=n_filters_2, stride_2=stride_2, n_kernel_2=n_kernel_2, ba=ba, avg_ba=avg_ba, ba_config=ba_config, tau=tau_scnn, thr=thr_scnn, avg_pool_1_stride=avg_pool_1_stride, avg_pool_1_k=avg_pool_1_k, avg_pool_2_stride=avg_pool_2_stride, avg_pool_2_k=avg_pool_2_k)\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        return self.core.zero_state(batch_size, tf.float32), \\\n",
    "               (tf.zeros((batch_size, self._num_actions)), tf.zeros((batch_size, 1))), \\\n",
    "               self.scnn.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    def initial_eligibility_traces(self, batch_size):\n",
    "        initial_eligibility_traces = [\n",
    "            tf.tile(tf.zeros_like(self.core.w_in_var[None, ..., None]), (batch_size, 1, 1, 2)),\n",
    "            tf.tile(tf.zeros_like(self.core.w_rec_var[None, ..., None]), (batch_size, 1, 1, 2))\n",
    "        ]\n",
    "        return initial_eligibility_traces\n",
    "\n",
    "    def _head(self, core_output, head_state, torso_dict):\n",
    "        def f(core_output):\n",
    "            i_policy_logits = snt.Linear(self._num_actions, name='policy_logits')(core_output)\n",
    "            i_baseline = tf.squeeze(snt.Linear(1, name='baseline')(core_output), axis=-1)\n",
    "            return i_policy_logits, i_baseline\n",
    "\n",
    "        core_output = tf.concat((core_output, torso_dict['c1_r'], torso_dict['c2_r']), -1)\n",
    "        policy = 0.\n",
    "        baseline = 0.\n",
    "        for decay in self.decay:\n",
    "            i_policy_logits, i_baseline = snt.BatchApply(f)(core_output)\n",
    "            policy += exp_convolve(i_policy_logits, decay, initializer=head_state[0])\n",
    "            baseline += exp_convolve(i_baseline[..., None], decay, initializer=head_state[1])\n",
    "        policy = policy[self.n_rnn_step_factor - 1::self.n_rnn_step_factor]\n",
    "        baseline = baseline[self.n_rnn_step_factor - 1::self.n_rnn_step_factor]\n",
    "\n",
    "        def g(policy):\n",
    "            new_action = tf.multinomial(policy, num_samples=1,\n",
    "                                        output_dtype=tf.int64)\n",
    "            new_action = tf.squeeze(new_action, 1, name='new_action')\n",
    "            return new_action\n",
    "\n",
    "        new_action = snt.BatchApply(g)(policy)\n",
    "        new_head_state = (policy[-1], baseline[-1])\n",
    "\n",
    "        return AgentOutput(new_action, policy, baseline[..., 0]), new_head_state\n",
    "\n",
    "    def _build(self, input_, core_state):\n",
    "        action, env_output = input_\n",
    "        env_outputs = environments.StepOutput(\n",
    "            reward=env_output.reward[None, ...],\n",
    "            info=nest.map_structure(lambda t: t[None, ...], env_output.info),\n",
    "            done=to_bool(tf.cast(env_output.done, tf.int64)[None, ...]),\n",
    "            observation=(tf.to_float(env_output.observation[0])[None, ...], tf.zeros(()))\n",
    "        )\n",
    "        actions = action[None, ...]\n",
    "        outputs, core_state, custom_rnn_output, torso_outputs = self.unroll(actions, env_outputs, core_state)\n",
    "        return nest.map_structure(lambda t: tf.squeeze(t, 0), outputs), core_state, \\\n",
    "               custom_rnn_output, torso_outputs\n",
    "\n",
    "    @snt.reuse_variables\n",
    "    def unroll(self, actions, env_outputs, core_state, write_to_collection=False):\n",
    "        _, _, done, _ = env_outputs\n",
    "\n",
    "        env_outputs = environments.StepOutput(\n",
    "            reward=env_outputs.reward,\n",
    "            info=env_outputs.info,\n",
    "            done=env_outputs.done,\n",
    "            observation=(env_outputs.observation[0], tf.zeros(()))\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        env_ouputs = tuple of images and blicket state (boolean)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        head_state = core_state[1]\n",
    "        n_time, n_batch = actions.get_shape()\n",
    "        done = tf.cast(done, tf.float32)[..., None]\n",
    "        expanded_dones = tf.reshape(\n",
    "            tf.tile(done[:, None, ...],\n",
    "                    (1, self.n_rnn_step_factor, 1, 1)), (n_time * self.n_rnn_step_factor, n_batch, -1))\n",
    "        frame = tf.cast(env_outputs.observation[0], tf.float32) / 255.\n",
    "        with tf.variable_scope('convnet'):\n",
    "            i_conv1 = snt.BatchApply(snt.Conv2D(self.n_filters_1, self.n_kernel_1, stride=self.stride_1, padding='VALID', use_bias=True))(frame)\n",
    "            shp = i_conv1.get_shape()\n",
    "            i_conv1 = tf.reshape(\n",
    "                tf.tile(i_conv1[:, None, ...],\n",
    "                        (1, self.n_rnn_step_factor, 1, 1, 1, 1)), (n_time * self.n_rnn_step_factor, n_batch, *shp[2:]))\n",
    "            i_conv1 = tf.transpose(i_conv1, (1, 0, 2, 3, 4))\n",
    "\n",
    "            scnn_output, new_scnn_state = tf.nn.dynamic_rnn(self.scnn, i_conv1, initial_state=core_state[2]) #look into this\n",
    "\n",
    "            to_collection = dict()\n",
    "            to_collection['lin_z'] = tf.zeros_like(scnn_output[1])\n",
    "            to_collection['lin_act'] = tf.zeros_like(scnn_output[1])\n",
    "            to_collection['c1_act'] = scnn_output[1]\n",
    "            to_collection['c1_z'] = scnn_output[0]\n",
    "            to_collection['c2_act'] = scnn_output[3]\n",
    "            to_collection['c2_z'] = scnn_output[2]\n",
    "            if self.ba or self.avg_ba:\n",
    "                to_collection['c1_r'] = tf.transpose(scnn_output[4], (1, 0, 2))\n",
    "                to_collection['c2_r'] = tf.transpose(scnn_output[5], (1, 0, 2))\n",
    "            if write_to_collection:\n",
    "                tf.add_to_collection('torso_output', to_collection)\n",
    "            torso_outputs = scnn_output[2]\n",
    "            expanded_dones = tf.transpose(expanded_dones, (1, 0, 2))\n",
    "            dynamic_rnn_inputs = tf.concat((scnn_output[2], expanded_dones), -1)\n",
    "\n",
    "        custom_rnn_output, core_state = tf.nn.dynamic_rnn(\n",
    "            self.core, dynamic_rnn_inputs, initial_state=core_state[0])\n",
    "        custom_rnn_output = nest.map_structure(switch_time_and_batch_dimension, custom_rnn_output)\n",
    "        core_output = custom_rnn_output[0]\n",
    "        core_output.set_shape((n_time * self.n_rnn_step_factor, n_batch, self.rnn_units))\n",
    "        head_output, head_state = self._head(core_output, head_state, to_collection)\n",
    "        core_state = (core_state, head_state, new_scnn_state)\n",
    "        return head_output, core_state, custom_rnn_output, torso_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_conv1 shape: (5, 10, 32, 32, 16)\n",
      "scnn state: (5, 32, 32, 16) (5, 7, 7, 32)\n",
      "WARNING:tensorflow:From C:\\Users\\saai2\\AppData\\Local\\Temp\\ipykernel_2776\\2124098421.py:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "scnn output shape: [TensorShape([5, 10, 16384]), TensorShape([5, 10, 16384]), TensorShape([5, 10, 1568]), TensorShape([5, 10, 1568])]\n",
      "new scnn state shape: [TensorShape([5, 32, 32, 16]), TensorShape([5, 7, 7, 32])]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_time = 10\n",
    "rnn_units = 100\n",
    "\n",
    "img = np.random.rand(batch_size, n_time, 128,128,4)\n",
    "img_tensor = tf.convert_to_tensor(img, tf.float32)\n",
    "i_conv1 = snt.BatchApply(snt.Conv2D(16, 8, stride=4))(img_tensor)\n",
    "conv_shape = i_conv1.get_shape()\n",
    "\n",
    "print('i_conv1 shape:', i_conv1.get_shape())\n",
    "\n",
    "scnn = SpikingCNN()\n",
    "scnn_state = scnn.zero_state(batch_size, tf.float32)\n",
    "\n",
    "print('scnn state:', scnn_state[0].get_shape(), scnn_state[1].get_shape())\n",
    "\n",
    "scnn_output, new_scnn_state = tf.compat.v1.nn.dynamic_rnn(scnn, i_conv1, initial_state=scnn_state)\n",
    "\n",
    "print('scnn output shape:', [o.get_shape() for o in scnn_output])\n",
    "print('new scnn state shape:', [s.get_shape() for s in new_scnn_state])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kernel_1 = 8\n",
    "n_kernel_2 = 8\n",
    "n_stride_1 = 4\n",
    "n_stride_2 = 4\n",
    "n_filters_2 = 32\n",
    "\n",
    "n_w_1 = (128 - n_kernel_1) // n_stride_1 + 2\n",
    "n_w_2 = (n_w_1 - n_kernel_2) // n_stride_2 + 1\n",
    "n_inputs = n_w_2 * n_w_2 * n_filters_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scnn_output 2 (5, 10, 1568)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "inputs: (5, 1568)\n",
      "w_in_val: (1568, 100)\n",
      "w_rec_val: (100, 100)\n",
      "z: (5, 100)\n",
      "rnn output: [TensorShape([5, 10, 100]), TensorShape([5, 10, 100, 2]), TensorShape([5, 10, 1]), TensorShape([5, 10, 1])]\n",
      "rnn state: [TensorShape([5, 100, 2]), TensorShape([5, 100]), TensorShape([5, 100])]\n",
      "core_output: (10, 5, 100)\n",
      "policy: (10, 5, 3)\n",
      "baseline: (10, 5, 1)\n",
      "WARNING:tensorflow:From c:\\Users\\saai2\\Anaconda3\\envs\\causalML\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "new action shape: (10, 5)\n",
      "new controller state: [TensorShape([5, 3]), TensorShape([5, 1])]\n"
     ]
    }
   ],
   "source": [
    "core = CustomALIF(n_in = n_inputs, n_rec = rnn_units)\n",
    "rnn_state = core.zero_state(batch_size, tf.float32)\n",
    "\n",
    "print('scnn_output 2', scnn_output[2].get_shape())\n",
    "\n",
    "rnn_output, rnn_state = tf.compat.v1.nn.dynamic_rnn(core, scnn_output[2], initial_state=rnn_state)\n",
    "\n",
    "print('rnn output:', [o.get_shape() for o in rnn_output])\n",
    "print('rnn state:', [s.get_shape() for s in rnn_state])\n",
    "\n",
    "rnn_output = nest.map_structure(switch_time_and_batch_dimension, rnn_output)\n",
    "core_output = rnn_output[0]\n",
    "core_output.set_shape((n_time * 1, batch_size, rnn_units))\n",
    "\n",
    "print('core_output:', core_output.get_shape())\n",
    "\n",
    "# print('4th and 5th output:', scnn_output[0][3], scnn_output[0][4])\n",
    "\n",
    "num_actions = 3\n",
    "tau = 1\n",
    "decays = [np.exp(-1.0 / tau)]\n",
    "\n",
    "controller_state = (tf.zeros((batch_size, num_actions)), tf.zeros((batch_size, 1)))\n",
    "\n",
    "def f(core_output):\n",
    "    i_policy_logits = snt.Linear(num_actions, name='policy_logits')(core_output)\n",
    "    i_baseline = tf.squeeze(snt.Linear(1, name='baseline')(core_output), axis=-1)\n",
    "    return i_policy_logits, i_baseline\n",
    "\n",
    "policy = 0.\n",
    "baseline = 0.\n",
    "for decay in decays:\n",
    "    i_policy_logits, i_baseline = (f(core_output))\n",
    "    policy += exp_convolve(i_policy_logits, decay, initializer=controller_state[0])\n",
    "    baseline += exp_convolve(i_baseline[..., None], decay, initializer=controller_state[1])\n",
    "policy = policy[1 - 1::1]\n",
    "baseline = baseline[1 - 1::1]\n",
    "\n",
    "print('policy:', policy.get_shape())\n",
    "print('baseline:', baseline.get_shape())\n",
    "\n",
    "def g(policy):\n",
    "    new_action = tf.compat.v1.multinomial(policy, num_samples=1,\n",
    "                                        output_dtype=tf.int64)\n",
    "    new_action = tf.squeeze(new_action, 1, name='new_action')\n",
    "    return new_action\n",
    "\n",
    "new_action = (g(policy))\n",
    "# print(new_action.eval())\n",
    "new_controller_state = (policy[-1], baseline[-1])\n",
    "\n",
    "print('new action shape:', new_action.get_shape())\n",
    "print('new controller state:', [s.get_shape() for s in new_controller_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as run_session:\n",
    "    batch_size = 5\n",
    "    n_time = 10\n",
    "    rnn_units = 100\n",
    "\n",
    "    #img = np.random.rand(batch_size, n_time, 128,128,4)\n",
    "    #img_tensor = tf.convert_to_tensor(img, tf.float32)\n",
    "    i_conv1 = snt.BatchApply(snt.Conv2D(16, 8, stride=4))(image1)\n",
    "    conv_shape = i_conv1.get_shape()\n",
    "\n",
    "\n",
    "    #i_conv1 = tf.transpose(i_conv1, (1, 0, 2, 3, 4))\n",
    "\n",
    "    # temp = i_conv1[:, None, ...]\n",
    "    # print('temp:', temp.get_shape())\n",
    "\n",
    "    print('i_conv1 shape:', i_conv1.get_shape())\n",
    "\n",
    "    scnn = SpikingCNN()\n",
    "    scnn_state = scnn.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    print('scnn state:', scnn_state[0].get_shape(), scnn_state[1].get_shape())\n",
    "\n",
    "    scnn_output, new_scnn_state = tf.nn.dynamic_rnn(scnn, i_conv1, initial_state=scnn_state)\n",
    "\n",
    "    print('scnn output shape:', [o.get_shape() for o in scnn_output])\n",
    "    print('new scnn state shape:', [s.get_shape() for s in new_scnn_state])\n",
    "\n",
    "    core = CustomALIF(n_in = n_inputs, n_rec = rnn_units)\n",
    "    rnn_state = core.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    print('scnn_output 2', scnn_output[2].get_shape())\n",
    "\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(core, scnn_output[2], initial_state=rnn_state)\n",
    "\n",
    "    print('rnn output:', [o.get_shape() for o in rnn_output])\n",
    "    print('rnn state:', [s.get_shape() for s in rnn_state])\n",
    "\n",
    "    rnn_output = nest.map_structure(switch_time_and_batch_dimension, rnn_output)\n",
    "    core_output = rnn_output[0]\n",
    "    core_output.set_shape((n_time * 1, batch_size, rnn_units))\n",
    "\n",
    "    print('core_output:', core_output.get_shape())\n",
    "\n",
    "    # print('4th and 5th output:', scnn_output[0][3], scnn_output[0][4])\n",
    "\n",
    "    num_actions = 2\n",
    "    tau = 1\n",
    "    decays = [np.exp(-1.0 / tau)]\n",
    "\n",
    "    controller_state = (tf.zeros((batch_size, num_actions)), tf.zeros((batch_size, 1)))\n",
    "\n",
    "    def f(core_output):\n",
    "        i_policy_logits = snt.Linear(num_actions, name='policy_logits')(core_output)\n",
    "        i_baseline = tf.squeeze(snt.Linear(1, name='baseline')(core_output), axis=-1)\n",
    "        return i_policy_logits, i_baseline\n",
    "\n",
    "    policy = 0.\n",
    "    baseline = 0.\n",
    "    for decay in decays:\n",
    "        i_policy_logits, i_baseline = snt.BatchApply(f)(core_output)\n",
    "        policy += exp_convolve(i_policy_logits, decay, initializer=controller_state[0])\n",
    "        baseline += exp_convolve(i_baseline[..., None], decay, initializer=controller_state[1])\n",
    "    policy = policy[1 - 1::1]\n",
    "    baseline = baseline[1 - 1::1]\n",
    "\n",
    "    print('policy:', policy.get_shape())\n",
    "    print('baseline:', baseline.get_shape())\n",
    "\n",
    "    def g(policy):\n",
    "        new_action = tf.distributions.Bernoulli(policy).sample()\n",
    "        #new_action = tf.cast(new_action, tf.float32, name='new_action')\n",
    "        #new_action = tf.squeeze(new_action, 1, name='new_action')\n",
    "        print('new action shape', new_action[0])\n",
    "        return new_action\n",
    "\n",
    "    new_action = snt.BatchApply(g)(policy)\n",
    "    # print(new_action.eval())\n",
    "    new_controller_state = (policy[-1], baseline[-1])\n",
    "\n",
    "    print('new action shape:', new_action.get_shape())\n",
    "    print('new controller state:', [s.get_shape() for s in new_controller_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rearranging code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters \n",
    "\n",
    "n_kernel_1 = 8\n",
    "n_kernel_2 = 8\n",
    "n_stride_1 = 4\n",
    "n_stride_2 = 4\n",
    "n_filters_2 = 32\n",
    "batch_size = 5\n",
    "n_time = 10\n",
    "rnn_units = 100\n",
    "\n",
    "img = np.random.rand(batch_size, n_time, 128,128,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "class SpikingAgent(snt.RNNCore):\n",
    "    def __init__(self, action_set, batch_size, rnn_units, n_rnn_step_factor=1,\n",
    "                 tau=20, tau_readout=5, thr=.615, n_filters_1=16, n_filters_2=32,\n",
    "                 n_kernel_1=8, n_kernel_2=8, stride_1=4, stride_2=4,\n",
    "                 beta=.16, tau_adaptation=300, fraction_adaptive=.4, n_refractory=3,\n",
    "                 tau_scnn=1., thr_scnn=1., n_time = 10):\n",
    "\n",
    "        self._num_actions = len(action_set)\n",
    "        self._batch_size = batch_size\n",
    "        self.rnn_units = rnn_units\n",
    "        tau_readout = np.atleast_1d(tau_readout)\n",
    "        self.decay = np.exp(-1 / tau_readout)\n",
    "        self.n_rnn_step_factor = n_rnn_step_factor\n",
    "        self.thr = thr\n",
    "        self.n_filters_1 = n_filters_1\n",
    "        self.n_kernel_1 = n_kernel_1\n",
    "        self.stride_1 = stride_1\n",
    "        self.n_filters_2 = n_filters_2\n",
    "        self.n_kernel_2 = n_kernel_2\n",
    "        self.stride_2 = stride_2\n",
    "        self.n_time = n_time\n",
    "\n",
    "        n_w_1 = (128 - n_kernel_1) // n_stride_1 + 2\n",
    "        n_w_2 = (n_w_1 - n_kernel_2) // n_stride_2 + 1\n",
    "        n_inputs = n_w_2 * n_w_2 * n_filters_2\n",
    "\n",
    "        self.core = CustomALIF(n_in = n_inputs, n_rec = rnn_units)\n",
    "\n",
    "        self.scnn = SpikingCNN()\n",
    "        self.scnn_state = scnn.zero_state(self._batch_size, tf.float32)\n",
    "    \n",
    "    def initial_state(self, batch_size, dtype=tf.float32):\n",
    "        return (tf.zeros((batch_size, self._num_actions)), tf.zeros((batch_size, 1)))\n",
    "    \n",
    "    def _controller(self, core_output, controller_state):\n",
    "        def f(core_output):\n",
    "            i_policy_logits = snt.Linear(self._num_actions, name='policy_logits')(core_output)\n",
    "            i_baseline = tf.squeeze(snt.Linear(1, name='baseline')(core_output), axis=-1)\n",
    "            return i_policy_logits, i_baseline\n",
    "\n",
    "        policy = 0.\n",
    "        baseline = 0.\n",
    "        for decay in decays:\n",
    "            i_policy_logits, i_baseline = snt.BatchApply(f)(core_output)\n",
    "            policy += exp_convolve(i_policy_logits, decay, initializer=controller_state[0])\n",
    "            baseline += exp_convolve(i_baseline[..., None], decay, initializer=controller_state[1])\n",
    "        policy = policy[self.n_rnn_step_factor - 1::self.n_rnn_step_factor]\n",
    "        baseline = baseline[self.n_rnn_step_factor - 1::self.n_rnn_step_factor]\n",
    "\n",
    "        def g(policy):\n",
    "            new_action = tf.compat.v1.multinomial(policy, num_samples=1,\n",
    "                                        output_dtype=tf.int64)\n",
    "            new_action = tf.squeeze(new_action, 1, name='new_action')\n",
    "            return new_action\n",
    "\n",
    "        new_action = snt.BatchApply(g)(policy)\n",
    "        new_controller_state = (policy[-1], baseline[-1])\n",
    "\n",
    "        return new_action, new_controller_state\n",
    "    \n",
    "    @snt.reuse_variables\n",
    "    def scnn_unroll(self, inputs):\n",
    "        i_conv1 = snt.BatchApply(snt.Conv2D(self.n_filters_1, self.n_kernel_1, stride=self.stride_1))(inputs)\n",
    "        scnn_output, new_scnn_state = tf.compat.v1.nn.dynamic_rnn(self.scnn, i_conv1, initial_state=self.scnn_state)\n",
    "\n",
    "        # rnn_state = core.zero_state(self._batch_size, tf.float32)\n",
    "        # rnn_output, rnn_state = tf.nn.dynamic_rnn(self.core, scnn_output[2], initial_state=rnn_state)\n",
    "\n",
    "        # rnn_output = nest.map_structure(switch_time_and_batch_dimension, rnn_output)\n",
    "        # core_output = rnn_output[0]\n",
    "        # core_output.set_shape((self.n_time * self.n_rnn_step_factor, self._batch_size, self.rnn_units))\n",
    "    \n",
    "        # initial_controller_state = self.initial_state(self._batch_size)\n",
    "        # controller_output, controller_state = self._controller(core_output, initial_controller_state)\n",
    "    \n",
    "        return scnn_output, new_scnn_state\n",
    "    \n",
    "    def context_pass(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: scnn output for the 6 context images\n",
    "        \"\"\"\n",
    "\n",
    "        rnn_state = core.zero_state(self._batch_size, tf.float32)\n",
    "        rnn_output, rnn_state = tf.nn.dynamic_rnn(self.core, scnn_output[2], initial_state=rnn_state)\n",
    "\n",
    "        rnn_output = nest.map_structure(switch_time_and_batch_dimension, rnn_output)\n",
    "        core_output = rnn_output[0]\n",
    "        core_output.set_shape((self.n_time * self.n_rnn_step_factor, self._batch_size, self.rnn_units))\n",
    "    \n",
    "    def query_pass(self, query_input, rnn_state):\n",
    "        \"\"\"\n",
    "        inputs: scnn output for the query image\n",
    "        \"\"\"\n",
    "\n",
    "        rnn_state = core.zero_state(self._batch_size, tf.float32)\n",
    "        rnn_output, rnn_state = tf.nn.dynamic_rnn(self.core, scnn_output[2], initial_state=rnn_state)\n",
    "\n",
    "        rnn_output = nest.map_structure(switch_time_and_batch_dimension, rnn_output)\n",
    "        core_output = rnn_output[0]\n",
    "\n",
    "        initial_controller_state = self.initial_state(self._batch_size)\n",
    "        controller_output, controller_state = self._controller(core_output, initial_controller_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_outputs, final_agent_state, custom_rnn_output, torso_outputs = agent.unroll(\n",
    "        agent_outputs.action, env_outputs,\n",
    "        agent_state, write_to_collection=True)\n",
    "\n",
    "rnn_v = custom_rnn_output[1][..., 0]\n",
    "rnn_thr = FLAGS.thr + agent.core.beta * custom_rnn_output[1][..., 1]\n",
    "rnn_pos = tf.nn.relu(rnn_v - rnn_thr)\n",
    "rnn_neg = tf.nn.relu(-rnn_v - rnn_thr)\n",
    "voltage_reg_rnn = tf.reduce_sum(tf.reduce_mean(tf.square(rnn_pos), 1))\n",
    "voltage_reg_rnn += tf.reduce_sum(tf.reduce_mean(tf.square(rnn_neg), 1))\n",
    "rnn_rate = tf.reduce_mean(custom_rnn_output[0], (0, 1))\n",
    "rnn_mean_rate = tf.reduce_mean(rnn_rate)\n",
    "analysis_tensors['rnn_rate'] = rnn_mean_rate\n",
    "rate_loss = tf.reduce_sum(tf.square(rnn_rate - .02)) * 1.\n",
    "torso_from_collection = tf.get_collection('torso_output')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Output'):\n",
    "    w_out_init = rd.randn(FLAGS.n_lstm * 2, dataset.n_phns) / np.sqrt(FLAGS.n_lstm * 2)  # original\n",
    "    w_out = tf.Variable(w_out_init, dtype=tf.float32)\n",
    "    if FLAGS.eprop in ['random']:\n",
    "        BA_out = tf.constant(rd.randn(FLAGS.n_lstm * 2, dataset.n_phns),\n",
    "                             dtype=tf.float32, name='BroadcastWeights')\n",
    "\n",
    "        BA_out = tf.get_variable(name=\"BAout\", initializer=BA_out, dtype=tf.float32)\n",
    "        phn_logits = BA_logits(outputs, w_out, BA_out)\n",
    "    elif FLAGS.eprop in ['adaptive']:\n",
    "        BA_out = tf.constant(rd.randn(FLAGS.n_lstm * 2, dataset.n_phns) / np.sqrt(FLAGS.n_lstm * 2),\n",
    "                             dtype=tf.float32, name='BroadcastWeights')\n",
    "\n",
    "        BA_out = tf.get_variable(name=\"BAout\", initializer=BA_out, dtype=tf.float32)\n",
    "        phn_logits = BA_logits(outputs, w_out, BA_out)\n",
    "    else:\n",
    "        phn_logits = einsum_bij_jk_to_bik(outputs, w_out)\n",
    "    b_out = tf.Variable(np.zeros(dataset.n_phns), dtype=tf.float32)\n",
    "    phn_logits = phn_logits + b_out\n",
    "\n",
    "\n",
    "# Define the graph for the loss function and the definition of the error\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=phns, logits=phn_logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    if FLAGS.l2 > 0:\n",
    "        losses_l2 = [tf.reduce_sum(tf.square(w)) for w in tf.trainable_variables()]\n",
    "        loss += FLAGS.l2 * tf.reduce_sum(losses_l2)\n",
    "\n",
    "    phn_prediction = tf.argmax(phn_logits, axis=2)\n",
    "    is_correct = tf.equal(phns, phn_prediction)\n",
    "    is_correct_float = tf.cast(is_correct, dtype=tf.float32)\n",
    "    ler = tf.reduce_sum(is_correct_float * weighted_relevant_mask, axis=1)\n",
    "    ler = 1. - tf.reduce_mean(ler)\n",
    "\n",
    "# Define the training step operation\n",
    "with tf.name_scope('Train'):\n",
    "    if not FLAGS.adam:\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "    else:\n",
    "        train_step = tf.train.AdamOptimizer(lr, epsilon=FLAGS.adam_epsilon).minimize(loss, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define CNN model\n",
    "cnn_model = keras.Sequential(...)\n",
    "cnn_model.compile(...)\n",
    "\n",
    "# Define RNN model\n",
    "rnn_model = keras.Sequential(...)\n",
    "rnn_model.compile(...)\n",
    "\n",
    "# Define controller function\n",
    "def controller(hidden_state):\n",
    "    # Define the logic for predicting actions\n",
    "    ...\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss(y_true, y_pred, cnn_output, rnn_output):\n",
    "    # Calculate individual losses for each model\n",
    "    cnn_loss = ...\n",
    "    rnn_loss = ...\n",
    "    controller_loss = ...\n",
    "\n",
    "    # Calculate total loss\n",
    "    total_loss = cnn_loss + rnn_loss + controller_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a training step function\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Pass images through CNN\n",
    "        cnn_output = cnn_model(images)\n",
    "\n",
    "        # Pass CNN output through RNN\n",
    "        rnn_output = rnn_model(cnn_output)\n",
    "\n",
    "        # Predict actions using controller function\n",
    "        actions = controller(rnn_output)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = custom_loss(labels, actions, cnn_output, rnn_output)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(loss, cnn_model.trainable_variables + rnn_model.trainable_variables)\n",
    "\n",
    "    # Update CNN and RNN models separately\n",
    "    cnn_optimizer.apply_gradients(zip(gradients[:len(cnn_model.trainable_variables)], cnn_model.trainable_variables))\n",
    "    rnn_optimizer.apply_gradients(zip(gradients[len(cnn_model.trainable_variables):], rnn_model.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_images, batch_labels in train_dataset:\n",
    "        train_step(batch_images, batch_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(128), Dimension(128), Dimension(4)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the images. Creating a loop to load all the images\n",
    "images = []\n",
    "for i in range(10):\n",
    "    images.append(mpimg.imread(\"train\\ACRE_train_000000_0\" + str(i) + \".png\"))\n",
    "\n",
    "#convert the images to tensors\n",
    "tensors = []\n",
    "for i in range(10):\n",
    "    tensors.append(tf.convert_to_tensor(images[i]))\n",
    "\n",
    "# Stack the image tensors along a new dimension\n",
    "images_tensor = tf.stack(tensors)\n",
    "\n",
    "# The images_tensor now has shape (10, 240, 320, 4), which represents 10 images of size 240x320, with 4 color channels per pixel.\n",
    "\n",
    "# Resize the images to a specific size (e.g., 224x224)\n",
    "resized_images = tf.image.resize_images(images_tensor, [128, 128])\n",
    "\n",
    "# Convert the image data type to float32\n",
    "float_images = tf.cast(resized_images, tf.float32)\n",
    "\n",
    "# Normalize the image pixels to the range [0, 1]\n",
    "normalized_images = float_images / 255.0\n",
    "\n",
    "normalized_images.shape # (10, 240, 240, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "7\n",
      "1568\n"
     ]
    },
    {
     "ename": "IncompatibleShapeError",
     "evalue": "Input Tensor must have rank 4 corresponding to data_format NHWC, but instead was (5, 10, 128, 128, 4) of rank 5.\n\noriginally defined at:\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\", line 1758, in __init__\n    custom_getter=custom_getter, name=name)\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\", line 464, in __init__\n    super(_ConvND, self).__init__(custom_getter=custom_getter, name=name)\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\base.py\", line 180, in __init__\n    custom_getter_=self._custom_getter)\n  File \"C:\\Users\\saai2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\template.py\", line 160, in make_template\n    **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompatibleShapeError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8140c27659ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mi_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv1:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_conv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\base.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capture_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m       \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgraph_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_connected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\template.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m       with variable_scope.variable_scope(\n\u001b[0;32m    383\u001b[0m           self._variable_scope, reuse=not self._first_call):\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m       \u001b[1;31m# The scope was not created at construction time, so create it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\template.py\u001b[0m in \u001b[0;36m_call_func\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m           \u001b[1;31m# Trackable).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mtrackable_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\base.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_build\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;31m# Make a dummy subscope to check the name scope we are in. We could read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;31m# the name scope from one of the outputs produced, except that the outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    549\u001b[0m           \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \"\"\"\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0m_verify_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_channel_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_channel_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\u001b[0m in \u001b[0;36m_verify_inputs\u001b[1;34m(inputs, channel_index, data_format)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;34m\"Input Tensor must have rank {} corresponding to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;34m\"data_format {}, but instead was {} of rank {}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             len(data_format), data_format, input_shape, len(input_shape)))\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m   \u001b[1;31m# Check type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompatibleShapeError\u001b[0m: Input Tensor must have rank 4 corresponding to data_format NHWC, but instead was (5, 10, 128, 128, 4) of rank 5.\n\noriginally defined at:\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\", line 1758, in __init__\n    custom_getter=custom_getter, name=name)\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\conv.py\", line 464, in __init__\n    super(_ConvND, self).__init__(custom_getter=custom_getter, name=name)\n  File \"c:\\Users\\saai2\\Anaconda3\\envs\\reward-based-e-prop\\lib\\site-packages\\sonnet\\python\\modules\\base.py\", line 180, in __init__\n    custom_getter_=self._custom_getter)\n  File \"C:\\Users\\saai2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\template.py\", line 160, in make_template\n    **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# #Maell's code\n",
    "\n",
    "# def pad(effective_kernel_size):\t # pylint: disable=unused-argument\n",
    "# \t\"\"\"No padding.\"\"\"\n",
    "# \treturn [2, 2]\n",
    "\n",
    "# print(n_w_1)\n",
    "# print(n_w_2)\n",
    "# print(n_inputs)\n",
    "\n",
    "# img = np.random.rand(5, 10,128,128,4)\n",
    "# img_tensor = tf.convert_to_tensor(img, tf.float32)\n",
    "# i_conv1 = snt.Conv2D(16, 8, stride=4)(img_tensor)\n",
    "# print('conv1:', i_conv1.get_shape())\n",
    "\t\n",
    "# scnn = SpikingCNN()\n",
    "# scnn_state = scnn.zero_state(10, tf.float32)\n",
    "\n",
    "# scnn_output, scnn_state = scnn(i_conv1, scnn_state)\n",
    "# print([o.get_shape() for o in scnn_output])\n",
    "# print([s.get_shape() for s in scnn_state])\n",
    "\n",
    "# core = CustomALIF(n_in = n_inputs, n_rec = 100)\n",
    "# rnn_state = core.zero_state(10, tf.float32)\n",
    "\n",
    "# rnn_output, rnn_state = core(scnn_output[2], rnn_state)\n",
    "\n",
    "# print([o.get_shape() for o in rnn_output])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reward-based-e-prop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
